import os
import can
import cantools
from typing import List, Dict, Any, Optional, Tuple, TypeAlias, Union
from cantools.database import Database
from tqdm import tqdm
from multiprocessing import Pool, cpu_count
import numpy as np
from collections import defaultdict
import yaml
from pathlib import Path
import yaml
from pathlib import Path
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
# å°è¯•å¯¼å…¥numbaç”¨äºJITåŠ é€Ÿ
try:
    from numba import jit

    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False

    # å¦‚æœnumbaä¸å¯ç”¨ï¼Œä½¿ç”¨ç©ºè£…é¥°å™¨
    def jit(*args, **kwargs):
        def decorator(func):
            return func

        return decorator if args and callable(args[0]) else decorator


StringPathLike: TypeAlias = Union[str, os.PathLike]

# if platform.system() == "Windows":
#     ENCODING = "gbk"
# else:
ENCODING = "utf-8"

# å¤§æ–‡ä»¶é˜ˆå€¼ï¼ˆå•ä½ï¼šå­—èŠ‚ï¼‰
LARGE_FILE_THRESHOLD = 500 * 1024 * 1024  # 500MB
VERY_LARGE_FILE_THRESHOLD = 1024 * 1024 * 1024  # 1GB


def load_config_from_yaml(yaml_path: StringPathLike) -> Dict[str, Any]:
    """
    ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®

    Args:
        yaml_path: YAMLé…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        é…ç½®å­—å…¸
    """
    yaml_path = Path(yaml_path)
    if not yaml_path.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {yaml_path}")

    with open(yaml_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)

    # éªŒè¯å¿…éœ€çš„é…ç½®é¡¹
    required_keys = ["dbc_path", "can_data_path"]
    missing_keys = [key for key in required_keys if key not in config]
    if missing_keys:
        raise ValueError(f"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…éœ€é¡¹: {', '.join(missing_keys)}")

    # è®¾ç½®é»˜è®¤å€¼
    defaults = {
        "output_dir": "./decoded",
        "step": 0.02,
        "save_formats": [".parquet", ".csv"],
        "num_processes": None,
        "batch_size": 1000,
        "use_numba": True,
        "signal_names": None,
        "signal_mapping": None,
        "time_from_zero": False,  # True: ä»0å¼€å§‹ç´¢å¼•ï¼›False: ä½¿ç”¨åŸå§‹æ—¶é—´æˆ³
    }

    # åˆå¹¶é»˜è®¤å€¼
    for key, default_value in defaults.items():
        if key not in config or config[key] is None:
            config[key] = default_value

    # å¤„ç†signal_namesï¼ˆå¦‚æœæ˜¯ç©ºåˆ—è¡¨ï¼Œè®¾ä¸ºNoneè¡¨ç¤ºè§£ææ‰€æœ‰ä¿¡å·ï¼‰
    if isinstance(config["signal_names"], list) and len(config["signal_names"]) == 0:
        config["signal_names"] = None

    # è½¬æ¢save_formatsä¸ºå…ƒç»„
    if isinstance(config["save_formats"], list):
        config["save_formats"] = tuple(config["save_formats"])

    return config


@jit(nopython=False, cache=True, parallel=False)
def _fast_array_conversion(timestamps_list, values_list):
    """ä½¿ç”¨NumbaåŠ é€Ÿæ•°ç»„è½¬æ¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
    timestamps = np.array(timestamps_list, dtype=np.float64)
    values = np.array(values_list, dtype=np.float64)
    return timestamps, values


def _build_decoder_map(dbc_data: Database) -> Dict[int, Any]:
    """é¢„ç¼–è¯‘æ¶ˆæ¯IDåˆ°è§£ç å‡½æ•°çš„æ˜ å°„ï¼Œé¿å…è¿è¡Œæ—¶æŸ¥æ‰¾ã€‚"""
    decoder_map: Dict[int, Any] = {}
    for __msg in getattr(dbc_data, "messages", []):
        decoder_map[__msg.frame_id] = __msg.decode
    return decoder_map


def _process_single_file_wrapper(args):
    """
    å¤šè¿›ç¨‹wrapperå‡½æ•°ï¼Œç”¨äºå¤„ç†å•ä¸ªCANæ–‡ä»¶ã€‚
    å¿…é¡»åœ¨æ¨¡å—çº§åˆ«å®šä¹‰ä»¥æ”¯æŒmultiprocessingåºåˆ—åŒ–ã€‚
    ä¼˜åŒ–ï¼šæ‰¹é‡å¤„ç†ã€é¢„åˆ†é…å†…å­˜ã€å‡å°‘åˆ—è¡¨è¿½åŠ å¼€é”€ã€å¤§æ–‡ä»¶ä¼˜åŒ–
    """
    import traceback

    (
        dbc_url,
        log_file_path,
        file_type,
        signal_names,
        signal_corr,
        step,
        time_from_zero,
        save_dir,
        save_formats,
    ) = args

    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    try:
        file_size = os.path.getsize(log_file_path)
        is_large_file = file_size > LARGE_FILE_THRESHOLD
        is_very_large_file = file_size > VERY_LARGE_FILE_THRESHOLD

        if is_very_large_file:
            print(
                f"\nâš  è¶…å¤§æ–‡ä»¶: {os.path.basename(log_file_path)} ({file_size/1024/1024:.0f}MB)"
            )
            print(f"  ä½¿ç”¨ä¼˜åŒ–æ¨¡å¼å¤„ç†ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    except:
        file_size = 0
        is_large_file = False
        is_very_large_file = False

    # åœ¨å­è¿›ç¨‹ä¸­åŠ è½½DBCæ–‡ä»¶å¹¶é¢„ç¼–è¯‘è§£ç å‡½æ•°
    with open(dbc_url, "r", encoding=ENCODING) as f:
        dbc_data = cantools.db.load(f, database_format="dbc", strict=False)
    decoder_map = _build_decoder_map(dbc_data)

    # å¤„ç†CANæ–‡ä»¶
    try:
        # æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½æ—¥å¿—æ•°æ®
        if file_type == "blf":
            log_data = can.BLFReader(log_file_path)
        elif file_type == "asc":
            log_data = can.ASCReader(log_file_path)
        else:
            return None

        # è§£ç ä¿¡å· - ä½¿ç”¨ä¼˜åŒ–çš„æ•°æ®ç»“æ„ä¸é¢„ç¼–è¯‘è§£ç å‡½æ•°
        from asammdf import Signal

        decoded: Dict[str, Dict[str, list]] = {}
        signal_names_set = set(signal_names) if signal_names else None

        # ç»Ÿè®¡ä¿¡æ¯
        total_msgs = 0
        decoded_msgs = 0
        error_count = 0
        error_types = {}  # é”™è¯¯ç±»å‹ç»Ÿè®¡

        # æ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°
        if is_very_large_file:
            batch_size = 500  # è¶…å¤§æ–‡ä»¶ä½¿ç”¨å°æ‰¹æ¬¡
        elif is_large_file:
            batch_size = 800
        else:
            batch_size = 1000

        # æ‰¹é‡æ”¶é›†æ¶ˆæ¯æ•°æ®ï¼ˆå‡å°‘é¢‘ç¹çš„å­—å…¸æ“ä½œï¼‰
        temp_data = defaultdict(lambda: {"timestamps": [], "values": []})

        def flush_batch():
            """å°†ç´¯ç§¯çš„åˆ—è¡¨è½¬ä¸ºNumPyæ•°ç»„å¹¶åˆå¹¶åˆ°ä¸»å­˜å‚¨ã€‚"""
            if not temp_data:
                return
            for sig_name, data in temp_data.items():
                if not data["timestamps"]:
                    continue
                t_arr = np.asarray(data["timestamps"], dtype=np.float64)
                v_arr = np.asarray(data["values"], dtype=np.float64)
                bucket = decoded.setdefault(sig_name, {"timestamps": [], "values": []})
                bucket["timestamps"].append(t_arr)
                bucket["values"].append(v_arr)
            temp_data.clear()

        # æ‰¹é‡å¤„ç†æ¶ˆæ¯
        for __msg in log_data:
            total_msgs += 1
            decoder = decoder_map.get(__msg.arbitration_id)
            if decoder is None:
                error_count += 1
                error_types["UnknownMessage"] = error_types.get("UnknownMessage", 0) + 1
                continue

            try:
                __dec = decoder(__msg.data)
                if not __dec:
                    error_count += 1
                    continue

                decoded_msgs += 1
                for __k, __v in __dec.items():
                    if signal_names_set is None or __k in signal_names_set:
                        value = getattr(__v, "value", __v)
                        entry = temp_data[__k]
                        entry["timestamps"].append(__msg.timestamp)
                        entry["values"].append(value)

                # æ¯å¤„ç†batch_sizeæ¡æ¶ˆæ¯ï¼Œè½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶åˆå¹¶
                if total_msgs % batch_size == 0:
                    flush_batch()

                    # å¤§æ–‡ä»¶æ˜¾ç¤ºè¿›åº¦
                    if is_very_large_file and total_msgs % 50000 == 0:
                        decode_rate = (
                            decoded_msgs / total_msgs * 100 if total_msgs > 0 else 0
                        )
                        print(
                            f"  å·²å¤„ç† {total_msgs} æ¡æ¶ˆæ¯ (è§£ç æˆåŠŸç‡: {decode_rate:.1f}%)..."
                        )

            except Exception as e:
                # æ•è·æ‰€æœ‰è§£ç é”™è¯¯ä½†ä¸ä¸­æ–­å¤„ç†
                error_count += 1
                error_type = type(e).__name__
                error_types[error_type] = error_types.get(error_type, 0) + 1
                continue

        # å¤„ç†å‰©ä½™çš„æ‰¹æ¬¡æ•°æ®
        flush_batch()

        # æ„å»ºSignalå¯¹è±¡ - ä¼˜åŒ–ï¼šåˆ†æ‰¹è½¬ä¸ºæ•°ç»„åå†åˆå¹¶ï¼Œå‡å°‘ä¸­é—´å¯¹è±¡
        sigs = []
        total_data_points = 0

        for __k, __v in decoded.items():
            if len(__v["timestamps"]) > 0:  # åªå¤„ç†æœ‰æ•°æ®çš„ä¿¡å·
                timestamps = np.concatenate(__v["timestamps"]) if len(__v["timestamps"]) > 1 else __v["timestamps"][0]
                values = np.concatenate(__v["values"]) if len(__v["values"]) > 1 else __v["values"][0]
                signal_name = signal_corr.get(__k, __k) if signal_corr else __k
                sigs.append(
                    Signal(values, timestamps, name=str(signal_name), encoding="utf-8")
                )
                total_data_points += len(timestamps)

        # ä¼°ç®—å†…å­˜ä½¿ç”¨ï¼ˆæ¯ä¸ªæ•°æ®ç‚¹çº¦16å­—èŠ‚ï¼š8å­—èŠ‚timestamp + 8å­—èŠ‚valueï¼‰
        estimated_memory_mb = (total_data_points * 16) / 1024 / 1024

        if is_very_large_file and sigs:
            print(f"  ä¿¡å·æ•°é‡: {len(sigs)}")
            print(f"  æ•°æ®ç‚¹æ€»æ•°: {total_data_points}")
            print(f"  ä¼°ç®—å†…å­˜: {estimated_memory_mb:.1f} MB")

            if estimated_memory_mb > 2000:  # è¶…è¿‡2GB
                print(f"  âš  è­¦å‘Š: ä¼°ç®—å†…å­˜è¶…è¿‡ 2GBï¼Œå»ºè®®å¢å¤§stepå€¼")

        # ä¿å­˜ç»“æœ
        if sigs:
            from asammdf import MDF
            import scipy.io as sio

            try:
                if is_very_large_file:
                    print(f"  æ­£åœ¨ç”ŸæˆMDFå¯¹è±¡...")

                mdf = MDF()
                mdf.append(sigs)

                if is_very_large_file:
                    print(f"  æ­£åœ¨è½¬æ¢ä¸ºDataFrameï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
                    # è®¡ç®—é¢„æœŸçš„DataFrameå¤§å° - ä½¿ç”¨ä¿¡å·çš„æ—¶é—´è·¨åº¦
                    try:
                        # ä»å·²è§£ç çš„ä¿¡å·ä¸­è·å–æœ€å¤§æ—¶é—´æˆ³
                        max_timestamp = max(
                            sig.timestamps[-1]
                            for sig in sigs
                            if len(sig.timestamps) > 0
                        )
                        min_timestamp = min(
                            sig.timestamps[0] for sig in sigs if len(sig.timestamps) > 0
                        )
                        time_span = max_timestamp - min_timestamp
                        expected_rows = (
                            int(time_span / step) if step > 0 else total_data_points
                        )
                        expected_memory_mb = (
                            (expected_rows * len(sigs) * 8) / 1024 / 1024
                        )
                        print(f"  æ—¶é—´è·¨åº¦: {time_span:.1f}ç§’")
                        print(f"  é¢„æœŸè¡Œæ•°: ~{expected_rows:,}")
                        print(f"  é¢„æœŸå†…å­˜: ~{expected_memory_mb:.0f} MB")
                    except (ValueError, IndexError):
                        # å¦‚æœæ— æ³•è®¡ç®—æ—¶é—´è·¨åº¦ï¼Œè·³è¿‡è¿™äº›ä¿¡æ¯
                        pass

                # å¯¹äºè¶…å¤§æ–‡ä»¶ï¼Œä½¿ç”¨æ›´å¤§çš„rasteræ­¥é•¿å‡å°‘æ•°æ®ç‚¹
                if is_very_large_file and step < 0.01:
                    print(f"  âš  è¶…å¤§æ–‡ä»¶æ£€æµ‹ï¼Œå»ºè®®ä½¿ç”¨æ›´å¤§çš„stepå€¼ (>=0.05)")

                df = mdf.to_dataframe(raster=step, time_from_zero=time_from_zero)

                if is_very_large_file:
                    print(f"  DataFrameå¤§å°: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
                    print(
                        f"  å†…å­˜å ç”¨: {df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB"
                    )

            except MemoryError as e:
                return {
                    "file": os.path.basename(log_file_path),
                    "total_msgs": total_msgs,
                    "decoded_msgs": decoded_msgs,
                    "signals": len(sigs),
                    "data_points": total_data_points,
                    "estimated_memory_mb": estimated_memory_mb,
                    "success": False,
                    "error": f"å†…å­˜ä¸è¶³: è½¬æ¢DataFrameæ—¶å†…å­˜è€—å°½ (ä¼°ç®—éœ€è¦ {estimated_memory_mb:.0f}MB). å»ºè®®: 1)å¢å¤§stepå€¼è‡³{step*5:.3f}æˆ–æ›´å¤§ 2)ä½¿ç”¨signal_namesè¿‡æ»¤ä¿¡å· 3)å‡å°‘è¿›ç¨‹æ•°è‡³1",
                }
            except Exception as e:
                return {
                    "file": os.path.basename(log_file_path),
                    "total_msgs": total_msgs,
                    "success": False,
                    "error": f"DataFrameè½¬æ¢å¤±è´¥: {str(e)}",
                }

            base_filename = os.path.splitext(os.path.basename(log_file_path))[0]

            # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„ä¿å­˜å‚æ•°
            save_methods = {
                ".mat": lambda file_url: sio.savemat(
                    file_url, df.to_dict(orient="list"), do_compression=True  # å¯ç”¨å‹ç¼©
                ),
                ".csv": lambda file_url: df.to_csv(
                    file_url,
                    index=True,
                    chunksize=(
                        10000 if not is_very_large_file else 5000
                    ),  # å¤§æ–‡ä»¶ä½¿ç”¨æ›´å°å—
                ),
                ".parquet": lambda file_url: df.to_parquet(
                    file_url,
                    compression="snappy",
                    index=True,
                    engine="pyarrow",  # ä½¿ç”¨pyarrowå¼•æ“æ›´å¿«
                ),
            }

            save_errors = []
            for save_format in save_formats:
                __file_url = os.path.join(save_dir, f"{base_filename}{save_format}")
                save_method = save_methods.get(save_format)
                if save_method:
                    try:
                        if is_very_large_file:
                            print(f"  æ­£åœ¨ä¿å­˜ {save_format} æ ¼å¼...")
                        save_method(__file_url)
                    except Exception as e:
                        # è®°å½•é”™è¯¯ä½†ç»§ç»­å°è¯•å…¶ä»–æ ¼å¼
                        error_msg = f"{save_format}: {str(e)}"
                        save_errors.append(error_msg)
                        # å°è¯•é™çº§æ–¹æ¡ˆ
                        try:
                            if save_format == ".csv":
                                df.to_csv(__file_url, index=False)
                            elif save_format == ".parquet":
                                df.to_parquet(
                                    __file_url, compression="snappy", index=False
                                )
                        except Exception as e2:
                            save_errors.append(f"{save_format} fallback: {str(e2)}")

            # è¿”å›ç»Ÿè®¡ä¿¡æ¯
            result = {
                "file": os.path.basename(log_file_path),
                "total_msgs": total_msgs,
                "decoded_msgs": decoded_msgs,
                "error_count": error_count,
                "error_types": error_types,  # æ·»åŠ é”™è¯¯ç±»å‹ç»Ÿè®¡
                "signals": len(sigs),
                "success": True,
            }

            if save_errors:
                result["save_warnings"] = save_errors

            if is_very_large_file:
                print(f"  âœ“ å®Œæˆå¤„ç†: {os.path.basename(log_file_path)}")

            return result
        else:
            # æ²¡æœ‰æˆåŠŸè§£ç ä»»ä½•ä¿¡å·
            return {
                "file": os.path.basename(log_file_path),
                "total_msgs": total_msgs,
                "decoded_msgs": decoded_msgs,
                "error_count": error_count,
                "error_types": error_types,  # æ·»åŠ é”™è¯¯ç±»å‹ç»Ÿè®¡
                "signals": 0,
                "success": False,
                "error": "No valid signals decoded",
            }
    except MemoryError as e:
        # å†…å­˜ä¸è¶³é”™è¯¯
        return {
            "file": os.path.basename(log_file_path),
            "success": False,
            "error": f"å†…å­˜ä¸è¶³: {str(e)}. å»ºè®®: 1)å¢å¤§stepå€¼ 2)è¿‡æ»¤ä¿¡å· 3)å‡å°‘è¿›ç¨‹æ•°",
        }
    except KeyboardInterrupt:
        # ç”¨æˆ·ä¸­æ–­
        return {
            "file": os.path.basename(log_file_path),
            "success": False,
            "error": "ç”¨æˆ·ä¸­æ–­",
        }
    except Exception as e:
        # æ•è·æ‰€æœ‰å…¶ä»–å¼‚å¸¸ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
        error_detail = traceback.format_exc()
        return {
            "file": os.path.basename(log_file_path),
            "success": False,
            "error": f"{type(e).__name__}: {str(e)}",
            "traceback": error_detail[-500:],  # åªä¿ç•™æœ€å500å­—ç¬¦
        }


class CanDecoder:
    def __init__(
        self,
        dbc_url: StringPathLike,
        can_url: StringPathLike,
        use_numba: bool = True,  # æ˜¯å¦ä½¿ç”¨NumbaåŠ é€Ÿ
        batch_size: int = 1000,  # æ‰¹å¤„ç†å¤§å°
    ):  # æ„é€ å‡½æ•°ï¼Œåˆå§‹åŒ–å¯¹è±¡
        self.dbc_url = dbc_url  # å°†ä¼ å…¥çš„dbc_urlå‚æ•°èµ‹å€¼ç»™å¯¹è±¡çš„dbc_urlå±æ€§
        self.can_url = can_url  # å°†ä¼ å…¥çš„can_urlå‚æ•°èµ‹å€¼ç»™å¯¹è±¡çš„can_urlå±æ€§
        self.use_numba = use_numba and NUMBA_AVAILABLE  # åªæœ‰åœ¨å¯ç”¨æ—¶æ‰å¯ç”¨
        self.batch_size = batch_size  # æ‰¹å¤„ç†å¤§å°

        # æ€§èƒ½ç»Ÿè®¡
        self.performance_mode = True  # å¯ç”¨æ€§èƒ½ä¼˜åŒ–æ¨¡å¼

        self.dbcs = self.__load_dbc_multi(
            dbc_url
        )  # è°ƒç”¨ç§æœ‰æ–¹æ³•__load_dbc_multiåŠ è½½dbcæ–‡ä»¶ï¼Œå¹¶å°†ç»“æœèµ‹å€¼ç»™å¯¹è±¡çš„dbcså±æ€§
        self.blf_urls, self.asc_urls = self.__load_can_multi(
            can_url
        )  # è°ƒç”¨ç§æœ‰æ–¹æ³•__load_can_multiåŠ è½½canæ–‡ä»¶ï¼Œå¹¶å°†ç»“æœåˆ†åˆ«èµ‹å€¼ç»™å¯¹è±¡çš„blf_urlså’Œasc_urlså±æ€§

        # æ‰“å°æ€§èƒ½é…ç½®ä¿¡æ¯
        if self.use_numba:
            print("âœ“ Numba JITåŠ é€Ÿå·²å¯ç”¨")
        else:
            print("âš  Numbaä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å¼")
        print(f"âœ“ æ‰¹å¤„ç†å¤§å°: {self.batch_size}")

    @classmethod
    def from_config(cls, config_path: StringPathLike) -> "CanDecoder":
        """
        ä»YAMLé…ç½®æ–‡ä»¶åˆ›å»ºCanDecoderå®ä¾‹

        Args:
            config_path: YAMLé…ç½®æ–‡ä»¶è·¯å¾„

        Returns:
            CanDecoderå®ä¾‹

        Example:
            >>> decoder = CanDecoder.from_config('config.yaml')
            >>> decoder.read_can_files_multi()
        """
        config = load_config_from_yaml(config_path)

        print(f"\n{'='*60}")
        print(f"ä»é…ç½®æ–‡ä»¶åŠ è½½: {config_path}")
        print(f"{'='*60}")
        print(f"DBCæ–‡ä»¶: {config['dbc_path']}")
        print(f"æ•°æ®è·¯å¾„: {config['can_data_path']}")
        print(f"è¾“å‡ºç›®å½•: {config['output_dir']}")
        print(f"Stepå€¼: {config['step']}")

        if config["signal_names"]:
            print(f"ä¿¡å·è¿‡æ»¤: å¯ç”¨ ({len(config['signal_names'])} ä¸ªä¿¡å·)")
        else:
            print(f"ä¿¡å·è¿‡æ»¤: ç¦ç”¨ (è§£ææ‰€æœ‰ä¿¡å·)")

        print(f"{'='*60}\n")

        # åˆ›å»ºå®ä¾‹
        instance = cls(
            dbc_url=config["dbc_path"],
            can_url=config["can_data_path"],
            use_numba=config["use_numba"],
            batch_size=config["batch_size"],
        )

        # ä¿å­˜é…ç½®ä¾›åç»­ä½¿ç”¨
        instance._config = config

        return instance

    def run_from_config(self) -> None:
        """
        ä½¿ç”¨åŠ è½½çš„é…ç½®è¿è¡ŒCANæ–‡ä»¶å¤„ç†

        è¯¥æ–¹æ³•ä»…åœ¨é€šè¿‡from_config()åˆ›å»ºå®ä¾‹åå¯ç”¨
        """
        if not hasattr(self, "_config"):
            raise RuntimeError(
                "æ­¤æ–¹æ³•ä»…åœ¨é€šè¿‡from_config()åˆ›å»ºå®ä¾‹åå¯ç”¨ã€‚"
                "è¯·ä½¿ç”¨ CanDecoder.from_config('config.yaml') åˆ›å»ºå®ä¾‹ã€‚"
            )

        config = self._config

        self.read_can_files_multi(
            signal_names=config["signal_names"],
            signal_corr=config["signal_mapping"],
            step=config["step"],
            save_dir=config["output_dir"],
            save_formats=config["save_formats"],
            num_processes=config["num_processes"],
            time_from_zero=config["time_from_zero"],
        )

    def __load_dbc_single(self, dbc_url: StringPathLike) -> Tuple[str, Any]:
        """
        Load a DBC file and return the database object.
        """
        # æ‰“å¼€æŒ‡å®šè·¯å¾„çš„DBCæ–‡ä»¶ï¼Œä»¥åªè¯»æ¨¡å¼("r")å’ŒæŒ‡å®šçš„ç¼–ç æ ¼å¼(ENCODING)è¯»å–æ–‡ä»¶å†…å®¹
        with open(dbc_url, "r", encoding=ENCODING) as f:
            # ä½¿ç”¨cantoolsåº“çš„dbæ¨¡å—åŠ è½½DBCæ–‡ä»¶å†…å®¹ï¼ŒæŒ‡å®šæ–‡ä»¶æ ¼å¼ä¸º"dbc"ï¼Œå¹¶è®¾ç½®ä¸¥æ ¼æ¨¡å¼ä¸ºFalse
            dbc_content = cantools.db.load(f, database_format="dbc", strict=False)
        # è¿”å›DBCæ–‡ä»¶çš„è·¯å¾„å’ŒåŠ è½½çš„æ•°æ®åº“å¯¹è±¡ï¼Œç¡®ä¿dbc_urlæ˜¯å­—ç¬¦ä¸²
        return str(dbc_url), dbc_content

    def __load_dbc_multi(
        self,
        dbc_url: Union[StringPathLike, List[StringPathLike]],
    ) -> List[Tuple[str, Any]]:

        # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ç”¨äºå­˜å‚¨åŠ è½½çš„æ•°æ®åº“
        dbcs = []
        # æ£€æŸ¥dbc_urlçš„ç±»å‹ï¼Œå¦‚æœæ˜¯å­—ç¬¦ä¸²è·¯å¾„
        if isinstance(dbc_url, StringPathLike):
            # æ£€æŸ¥è¯¥è·¯å¾„æ˜¯å¦æ˜¯ä¸€ä¸ªç›®å½•
            if os.path.isdir(dbc_url):
                # è·å–ç›®å½•ä¸‹æ‰€æœ‰ä»¥.dbcç»“å°¾çš„æ–‡ä»¶è·¯å¾„
                dbc_urls = [
                    os.path.join(dbc_url, file)
                    for file in os.listdir(dbc_url)
                    if file.endswith(".dbc")
                ]
                # ä½¿ç”¨mapå‡½æ•°å¹¶è¡ŒåŠ è½½è¿™äº›.dbcæ–‡ä»¶
                dbcs.extend(map(self.__load_dbc_single, dbc_urls))

            # å¦‚æœæ˜¯ä¸€ä¸ªæ–‡ä»¶
            elif os.path.isfile(dbc_url):
                # å°†è¯¥æ–‡ä»¶è·¯å¾„æ·»åŠ åˆ°åˆ—è¡¨ä¸­
                dbc_urls = [dbc_url]
                # æ³¨é‡Šæ‰çš„ä»£ç ï¼šåŸæœ¬æ˜¯ç›´æ¥è°ƒç”¨__load_dbc_singleå‡½æ•°åŠ è½½å•ä¸ªæ–‡ä»¶
                # dbcs.append(__load_dbc_single(dbc_url))
            else:
                # å¦‚æœæ—¢ä¸æ˜¯ç›®å½•ä¹Ÿä¸æ˜¯æ–‡ä»¶ï¼ŒæŠ›å‡ºå¼‚å¸¸
                raise ValueError(f"Invalid DBC file path: {dbc_url}")
        # å¦‚æœdbc_urlæ˜¯åˆ—è¡¨
        elif isinstance(dbc_url, list):
            # è¿‡æ»¤å‡ºæ‰€æœ‰ä»¥.dbcç»“å°¾çš„æ–‡ä»¶è·¯å¾„ï¼Œå¹¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            dbc_urls = [str(url) for url in dbc_url if str(url).endswith(".dbc")]
            # æ³¨é‡Šæ‰çš„ä»£ç ï¼šåŸæœ¬æ˜¯ç›´æ¥è°ƒç”¨__load_dbc_singleå‡½æ•°åŠ è½½åˆ—è¡¨ä¸­çš„æ–‡ä»¶
            # dbcs.extend(map(__load_dbc_single, dbc_url))
        else:
            # å¦‚æœdbc_urlæ—¢ä¸æ˜¯å­—ç¬¦ä¸²ä¹Ÿä¸æ˜¯åˆ—è¡¨ï¼ŒæŠ›å‡ºå¼‚å¸¸
            raise ValueError(f"Invalid DBC file path: {dbc_url}")
        # å¯¼å…¥ThreadPoolExecutorç”¨äºå¹¶è¡Œå¤„ç†
        from concurrent.futures import ThreadPoolExecutor

        # ä½¿ç”¨ThreadPoolExecutorå¹¶è¡ŒåŠ è½½æ‰€æœ‰.dbcæ–‡ä»¶
        with ThreadPoolExecutor() as executor:
            dbcs = list(executor.map(self.__load_dbc_single, dbc_urls))
        # è¿”å›åŠ è½½çš„æ•°æ®åº“åˆ—è¡¨
        return dbcs

    def __load_can_multi(
        self,
        can_url: Union[StringPathLike, List[StringPathLike]],
    ) -> Tuple[List[StringPathLike], List[StringPathLike]]:
        """
        åŠ è½½å¤šä¸ª CAN æ–‡ä»¶è·¯å¾„ï¼Œå¹¶æ ¹æ®æ–‡ä»¶ç±»å‹ï¼ˆ.blf æˆ– .ascï¼‰åˆ†ç±»ã€‚

        Args:
            can_url (Union[StringPathLike, List[StringPathLike]]): CAN æ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„ï¼Œæˆ–åŒ…å«å¤šä¸ªè·¯å¾„çš„åˆ—è¡¨ã€‚

        Returns:
            Tuple[List[StringPathLike], List[StringPathLike]]: åŒ…å« .blf æ–‡ä»¶è·¯å¾„åˆ—è¡¨å’Œ .asc æ–‡ä»¶è·¯å¾„åˆ—è¡¨çš„å…ƒç»„ã€‚
        """
        blf_urls = []  # å­˜å‚¨æ‰€æœ‰ .blf æ–‡ä»¶è·¯å¾„çš„åˆ—è¡¨
        asc_urls = []  # å­˜å‚¨æ‰€æœ‰ .asc æ–‡ä»¶è·¯å¾„çš„åˆ—è¡¨

        def __process_path(path: StringPathLike):
            """å¤„ç†å•ä¸ªè·¯å¾„ï¼Œåˆ†ç±»ä¸º .blf æˆ– .asc æ–‡ä»¶"""
            if os.path.isdir(path):
                # å¦‚æœè·¯å¾„æ˜¯ç›®å½•ï¼Œåˆ—å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
                files = os.listdir(path)
                # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼å’Œè¿‡æ»¤å™¨ä¸€æ¬¡æ€§å¤„ç†æ–‡ä»¶
                files = os.listdir(path)
                blf_urls.extend(
                    os.path.join(path, file) for file in files if file.endswith(".blf")
                )
                asc_urls.extend(
                    os.path.join(path, file) for file in files if file.endswith(".asc")
                )
            elif os.path.isfile(path):
                # ä½¿ç”¨å­—å…¸æ˜ å°„å‡å°‘ if-else åˆ¤æ–­
                extension_map = {".blf": blf_urls, ".asc": asc_urls}
                ext = os.path.splitext(path)[1].lower()
                if ext in extension_map:
                    extension_map[ext].append(path)
            return blf_urls, asc_urls

        # å•ä¸ªè·¯å¾„
        if isinstance(can_url, (str, os.PathLike)):
            __process_path(can_url)
        # åˆ—è¡¨ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†
        elif isinstance(can_url, list):
            from concurrent.futures import ThreadPoolExecutor

            with ThreadPoolExecutor() as executor:
                executor.map(__process_path, can_url)
        else:
            raise ValueError(
                "can_url must be a string, PathLike, or a list of such objects."
            )

        return blf_urls, asc_urls

    def __decode_can(
        self,
        dbc_data,
        can_data,
        signal_names: Optional[List[str]] = None,
        signal_corr: Optional[Dict[str, str]] = None,
    ) -> List[Dict[str, Any]]:
        """
        Decode CAN data using the provided DBC data.
        ä¼˜åŒ–ï¼šæ‰¹é‡å¤„ç†ã€å‡å°‘å†…å­˜åˆ†é…ã€ä½¿ç”¨numpyåŠ é€Ÿ
        """
        from asammdf import Signal  # ä» asammdf åº“å¯¼å…¥ Signal ç±»

        decoder_map = _build_decoder_map(dbc_data)

        decoded = defaultdict(lambda: {"timestamps": [], "values": []})
        signal_names_set = set(signal_names) if signal_names else None

        batch_limit = self.batch_size if self.batch_size > 0 else 1000

        def flush_batch(temp_storage: Dict[str, Dict[str, list]]):
            if not temp_storage:
                return
            for sig_name, data in temp_storage.items():
                if not data["timestamps"]:
                    continue
                t_arr = np.asarray(data["timestamps"], dtype=np.float64)
                v_arr = np.asarray(data["values"], dtype=np.float64)
                bucket = decoded[sig_name]
                bucket["timestamps"].append(t_arr)
                bucket["values"].append(v_arr)
            temp_storage.clear()

        temp_data: Dict[str, Dict[str, list]] = defaultdict(lambda: {"timestamps": [], "values": []})
        batch_count = 0
        for __msg in can_data:
            decoder = decoder_map.get(__msg.arbitration_id)
            if decoder is None:
                continue
            try:
                __dec = decoder(__msg.data)
                if not __dec:
                    continue

                for __k, __v in __dec.items():
                    if signal_names_set is None or __k in signal_names_set:
                        value = getattr(__v, "value", __v)
                        entry = temp_data[__k]
                        entry["timestamps"].append(__msg.timestamp)
                        entry["values"].append(value)

                batch_count += 1
                if self.performance_mode and batch_count % batch_limit == 0:
                    flush_batch(temp_data)

            except (KeyError, ValueError, Exception):
                continue

        flush_batch(temp_data)

        sigs = []
        for __k, __v in decoded.items():
            if __v["timestamps"]:
                if self.use_numba:
                    try:
                        timestamps, values = _fast_array_conversion(
                            np.concatenate(__v["timestamps"]) if len(__v["timestamps"]) > 1 else __v["timestamps"][0],
                            np.concatenate(__v["values"]) if len(__v["values"]) > 1 else __v["values"][0],
                        )
                    except Exception:
                        timestamps = np.concatenate(__v["timestamps"]) if len(__v["timestamps"]) > 1 else __v["timestamps"][0]
                        values = np.concatenate(__v["values"]) if len(__v["values"]) > 1 else __v["values"][0]
                else:
                    timestamps = np.concatenate(__v["timestamps"]) if len(__v["timestamps"]) > 1 else __v["timestamps"][0]
                    values = np.concatenate(__v["values"]) if len(__v["values"]) > 1 else __v["values"][0]

                signal_name = signal_corr.get(__k, __k) if signal_corr else __k
                sigs.append(
                    Signal(values, timestamps, name=str(signal_name), encoding="utf-8")
                )

        return sigs

    def __save_to(
        self,
        dbc_file_url,
        can_file_url,
        signals,
        step: float = 0.002,
        time_from_zero: bool = True,
        save_dir: StringPathLike = r"./can_decoded",
        save_formats: Tuple[str, ...] = (".csv", ".parquet", ".mat"),
    ):
        """
        Save decoded CAN data to specified formats.

        Args:
            dbc_file_url (str): Path to the DBC file.
            can_file_url (str): Path to the CAN file.
            signals (list): Decoded signals.
            step (float): Raster step size.
            save_dir (str): Directory to save the output files.
            save_formats (tuple): File formats to save (e.g., .csv, .parquet, .mat).
        """
        # æ£€æŸ¥ä¿å­˜ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        os.makedirs(save_dir, exist_ok=True)

        # å¯¼å…¥asammdfåº“ä¸­çš„MDFç±»å’Œscipyåº“ä¸­çš„ioæ¨¡å—
        from asammdf import MDF
        import scipy.io as sio

        # å¦‚æœæ²¡æœ‰ä¿¡å·æ•°æ®ï¼Œç›´æ¥è¿”å›
        if not signals:
            return

        # åˆ›å»ºä¸€ä¸ªMDFå¯¹è±¡
        mdf = MDF()
        # å°†è§£ç åçš„ä¿¡å·æ·»åŠ åˆ°MDFå¯¹è±¡ä¸­
        mdf.append(signals)
        # å°†MDFå¯¹è±¡è½¬æ¢ä¸ºDataFrameï¼ŒæŒ‡å®šæ …æ ¼æ­¥é•¿
        df = mdf.to_dataframe(raster=step, time_from_zero=time_from_zero)

        # ç”ŸæˆåŸºç¡€æ–‡ä»¶åï¼Œç”±DBCæ–‡ä»¶åå’ŒCANæ–‡ä»¶åç»„åˆè€Œæˆ
        base_filename = os.path.splitext(os.path.basename(can_file_url))[0]

        # å®šä¹‰æ–‡ä»¶æ ¼å¼ä¸ä¿å­˜æ–¹æ³•çš„æ˜ å°„ - ä¼˜åŒ–ç‰ˆæœ¬
        save_methods = {
            ".mat": lambda file_url: sio.savemat(
                file_url,
                df.to_dict(orient="list"),
                do_compression=True,  # MATæ–‡ä»¶å¯ç”¨å‹ç¼©
            ),
            ".csv": lambda file_url: df.to_csv(
                file_url, index=False, chunksize=10000  # åˆ†å—å†™å…¥å¤§æ–‡ä»¶
            ),
            ".parquet": lambda file_url: df.to_parquet(
                file_url,
                compression="snappy",
                index=False,
                engine="pyarrow" if self._has_pyarrow() else "fastparquet",
            ),
        }

        # éå†ä¿å­˜æ ¼å¼å¹¶è°ƒç”¨å¯¹åº”çš„ä¿å­˜æ–¹æ³•
        for save_format in save_formats:
            # ç”Ÿæˆå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
            __file_url = os.path.join(save_dir, f"{base_filename}{save_format}")
            # è·å–å¯¹åº”çš„ä¿å­˜æ–¹æ³•
            save_method = save_methods.get(save_format)
            if save_method:
                try:
                    # è°ƒç”¨ä¿å­˜æ–¹æ³•
                    save_method(__file_url)
                except Exception as e:
                    # å¦‚æœä¼˜åŒ–æ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ–¹æ³•
                    if save_format == ".csv":
                        df.to_csv(__file_url, index=False)
                    elif save_format == ".parquet":
                        df.to_parquet(__file_url, compression="snappy", index=False)
                    elif save_format == ".mat":
                        sio.savemat(__file_url, df.to_dict(orient="list"))
            else:
                # å¦‚æœä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼ŒæŠ›å‡ºå¼‚å¸¸
                raise ValueError(f"Unsupported save format: {save_format}")

    def _has_pyarrow(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å®‰è£…äº†pyarrow"""
        try:
            import pyarrow

            return True
        except ImportError:
            return False

    def read_single_can(
        self,
        dbc_url: str,
        dbc_data: Database,
        log_file_path: str,
        file_type: str,
        signal_names: Optional[List[str]] = None,
        signal_corr: Optional[Dict[str, str]] = None,
        step: float = 0.002,
        time_from_zero: bool = True,
        save_dir: str = r"./can_decoded",
        save_formats: Tuple[str, ...] = (".csv", ".parquet", ".mat"),
    ) -> List[Dict[str, Any]] | None:
        """
        Process a single CAN file (BLF or ASC) and save the decoded data.

        Args:
            dbc_url (str): Path to the DBC file.
            dbc_data (Database): DBC database object.
            log_file_path (str): Path to the CAN log file.
            file_type (str): Type of the CAN file ("blf" or "asc").
            signal_names (Optional[List[str]]): List of signal names to decode.
            signal_corr (Optional[Dict[str, str]]): Signal name corrections.
            step (float): Raster step size.
            save_dir (str): Directory to save the output files.
            save_formats (Tuple[str, ...]): File formats to save (e.g., ".csv", ".parquet").
        """
        try:
            # æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½æ—¥å¿—æ•°æ®
            if file_type == "blf":
                log_data = can.BLFReader(log_file_path)
            elif file_type == "asc":
                log_data = can.ASCReader(log_file_path)
            else:
                raise ValueError(f"Unsupported file type: {file_type}")

            # è§£ç ä¿¡å·
            signals = self.__decode_can(dbc_data, log_data, signal_names, signal_corr)

            # ä¿å­˜è§£ç ç»“æœ
            self.__save_to(
                dbc_url,
                log_file_path,
                signals,
                step,
                time_from_zero,
                save_dir,
                save_formats,
            )
            return signals
        except Exception as e:
            print(f"Error processing file {log_file_path}: {e}")

    def read_can_files(
        self,
        signal_names: Optional[List[str]] = None,
        signal_corr: Optional[Dict[str, str]] = None,
        step: float = 0.002,
        time_from_zero: bool = True,
        save_dir: str = r"./can_decoded",
        save_formats: Tuple[str, ...] = (".csv", ".parquet", ".mat"),
    ) -> None:
        """
        Read CAN files and decode them using the provided DBC data (single-threaded).
        """

        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        os.makedirs(save_dir, exist_ok=True)

        # éå†æ¯ä¸ª DBC æ–‡ä»¶
        for __dbc_url, __dbc_data in self.dbcs:
            # å¤„ç† BLF æ–‡ä»¶
            for __blf_url in tqdm(
                self.blf_urls,
                desc=f"Processing BLF files for {os.path.basename(__dbc_url)}",
            ):
                self.read_single_can(
                    __dbc_url,
                    __dbc_data,
                    str(__blf_url),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    "blf",
                    signal_names,
                    signal_corr,
                    step,
                    time_from_zero,
                    save_dir,
                    save_formats,
                )

            # å¤„ç† ASC æ–‡ä»¶
            for __asc_url in tqdm(
                self.asc_urls,
                desc=f"Processing ASC files for {os.path.basename(__dbc_url)}",
            ):
                self.read_single_can(
                    __dbc_url,
                    __dbc_data,
                    str(__asc_url),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    "asc",
                    signal_names,
                    signal_corr,
                    step,
                    time_from_zero,
                    save_dir,
                    save_formats,
                )

    def read_can_files_multi(
        self,
        signal_names: Optional[List[str]] = None,
        signal_corr: Optional[Dict[str, str]] = None,
        step: float = 0.02,
        time_from_zero: bool = True,
        save_dir: str = r"./can_decoded",
        save_formats: Tuple[str, ...] = (".csv", ".parquet", ".mat"),
        num_processes: Optional[int] = None,
    ) -> None:
        """
        Read multiple CAN files and decode them using the provided DBC data (multi-process).

        Args:
            signal_names (Optional[List[str]]): List of signal names to decode.
            signal_corr (Optional[Dict[str, str]]): Signal name corrections.
            step (float): Raster step size.
            save_dir (str): Directory to save the output files.
            save_formats (Tuple[str, ...]): File formats to save (e.g., ".csv", ".parquet", ".mat").
            num_processes (Optional[int]): Number of processes to use. Default is CPU count - 1.
        """

        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        os.makedirs(save_dir, exist_ok=True)

        # æ„å»ºä»»åŠ¡åˆ—è¡¨ - åªä¼ é€’DBCæ–‡ä»¶è·¯å¾„è€ŒéDatabaseå¯¹è±¡ï¼ˆä¸å¯åºåˆ—åŒ–ï¼‰
        tasks = []
        for __dbc_url, _ in self.dbcs:
            for __blf_url in self.blf_urls:
                tasks.append(
                    (
                        __dbc_url,
                        __blf_url,
                        "blf",
                        signal_names,
                        signal_corr,
                        step,
                        time_from_zero,
                        save_dir,
                        save_formats,
                    )
                )
            for __asc_url in self.asc_urls:
                tasks.append(
                    (
                        __dbc_url,
                        __asc_url,
                        "asc",
                        signal_names,
                        signal_corr,
                        step,
                        time_from_zero,
                        save_dir,
                        save_formats,
                    )
                )

        # è®¾ç½®è¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°-1ï¼Œè‡³å°‘ä¸º1
        if num_processes is None:
            num_processes = max(1, cpu_count() - 1)

        # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†
        with Pool(processes=num_processes) as pool:
            results = list(
                tqdm(
                    pool.imap_unordered(
                        _process_single_file_wrapper,
                        tasks,
                    ),
                    total=len(tasks),
                    desc="Processing CAN files",
                )
            )

        # ç»Ÿè®¡å¤„ç†ç»“æœ
        success_count = sum(1 for r in results if r and r.get("success"))
        failed_count = len(results) - success_count

        # æ˜¾ç¤ºæ±‡æ€»ä¿¡æ¯
        print(f"\n\n{'='*60}")
        print(f"å¤„ç†å®Œæˆ: {success_count}/{len(results)} ä¸ªæ–‡ä»¶æˆåŠŸ")
        print(f"{'='*60}")

        if failed_count > 0:
            print(f"\nâš  è­¦å‘Š: {failed_count} ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥æˆ–æ— æœ‰æ•ˆæ•°æ®")
            print(f"\nå¤±è´¥æ–‡ä»¶è¯¦æƒ…:")
            for r in results:
                if r and not r.get("success"):
                    error_msg = r.get("error", "Unknown error")
                    print(f"\n  âœ– {r.get('file', 'Unknown')}")
                    print(f"    é”™è¯¯: {error_msg}")
                    # æ˜¾ç¤ºå †æ ˆè·Ÿè¸ªï¼ˆå¦‚æœæœ‰ï¼‰
                    if "traceback" in r:
                        print(f"    è¯¦ç»†: {r['traceback']}")

        # æ˜¾ç¤ºä¿å­˜è­¦å‘Š
        save_warnings = [
            r.get("save_warnings", []) for r in results if r and r.get("save_warnings")
        ]
        if save_warnings:
            print(f"\nâš  ä¿å­˜è­¦å‘Š:")
            for r in results:
                if r and r.get("save_warnings"):
                    print(f"  {r.get('file')}: {', '.join(r['save_warnings'])}")

        # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
        total_msgs = sum(r.get("total_msgs", 0) for r in results if r)
        decoded_msgs = sum(r.get("decoded_msgs", 0) for r in results if r)
        error_msgs = sum(r.get("error_count", 0) for r in results if r)
        total_data_points = sum(r.get("data_points", 0) for r in results if r)

        # æ±‡æ€»é”™è¯¯ç±»å‹ç»Ÿè®¡
        all_error_types = {}
        for r in results:
            if r and "error_types" in r:
                for err_type, count in r["error_types"].items():
                    all_error_types[err_type] = all_error_types.get(err_type, 0) + count

        if total_msgs > 0:
            print(f"\næ¶ˆæ¯ç»Ÿè®¡:")
            print(f"  æ€»æ¶ˆæ¯æ•°: {total_msgs:,}")
            print(f"  æˆåŠŸè§£ç : {decoded_msgs:,} ({decoded_msgs/total_msgs*100:.1f}%)")
            print(f"  è§£ç é”™è¯¯: {error_msgs:,} ({error_msgs/total_msgs*100:.1f}%)")
            if total_data_points > 0:
                print(f"  æ•°æ®ç‚¹æ€»æ•°: {total_data_points:,}")

            # æ˜¾ç¤ºé”™è¯¯ç±»å‹ç»Ÿè®¡
            if all_error_types:
                print(f"\né”™è¯¯ç±»å‹ç»Ÿè®¡:")
                for err_type, count in sorted(
                    all_error_types.items(), key=lambda x: x[1], reverse=True
                ):
                    print(f"  {err_type}: {count:,} ({count/error_msgs*100:.1f}%)")

            if error_msgs > 0:
                print(f"\næç¤º: è§£ç é”™è¯¯é€šå¸¸ç”±ä»¥ä¸‹åŸå› å¼•èµ·:")
                print(f"  1. DecodeError - DBCæ–‡ä»¶ä¸­çš„å¤šè·¯å¤ç”¨å™¨IDå®šä¹‰ä¸å®é™…æ•°æ®ä¸åŒ¹é…")
                print(f"  2. KeyError - æ¶ˆæ¯IDä¸åœ¨DBCæ–‡ä»¶ä¸­")
                print(f"  3. ValueError - CANæ¶ˆæ¯æ•°æ®æ ¼å¼é”™è¯¯æˆ–ä¸å®Œæ•´")
                print(f"  4. struct.error - æ•°æ®è§£åŒ…å¤±è´¥")
                print(f"  è¿™äº›é”™è¯¯å·²è¢«è‡ªåŠ¨è·³è¿‡ï¼Œä¸å½±å“å…¶ä»–æœ‰æ•ˆæ¶ˆæ¯çš„å¤„ç†ã€‚")

        # å¤§æ–‡ä»¶å¤„ç†å»ºè®®
        large_files = [r for r in results if r and r.get("total_msgs", 0) > 100000]
        if large_files:
            print(f"\nğŸ’¡ å¤§æ–‡ä»¶å¤„ç†å»ºè®®:")
            print(f"  - æ£€æµ‹åˆ°å¤§å‹æ–‡ä»¶ï¼Œå»ºè®®ä½¿ç”¨ step=0.05 æˆ–æ›´å¤§")
            print(f"  - è€ƒè™‘ä½¿ç”¨ signal_names è¿‡æ»¤ä¸éœ€è¦çš„ä¿¡å·")
            print(f"  - å¦‚é‡åˆ°å†…å­˜ä¸è¶³ï¼Œå¯å‡å°‘ num_processes å‚æ•°")


def main():
    # è·å–é…ç½®æ–‡ä»¶è·¯å¾„
    if len(sys.argv) > 1:
        config_file = sys.argv[1]
    else:
        config_file = "config.yaml"

    config_path = Path(config_file)

    if not config_path.exists():
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        print(f"\nè¯·åˆ›å»ºé…ç½®æ–‡ä»¶æˆ–æŒ‡å®šæ­£ç¡®çš„è·¯å¾„ã€‚")
        print(f"ç”¨æ³•: python run_with_config.py [config_file]")
        return 1

    try:
        # ä»é…ç½®æ–‡ä»¶åˆ›å»ºè§£ç å™¨å®ä¾‹
        decoder = CanDecoder.from_config(config_path)

        # è¿è¡Œè§£æ
        decoder.run_from_config()

        print("\n" + "=" * 60)
        print("âœ“ å¤„ç†å®Œæˆï¼")
        print("=" * 60)

        return 0

    except FileNotFoundError as e:
        print(f"\né”™è¯¯: {e}")
        return 1
    except ValueError as e:
        print(f"\né…ç½®é”™è¯¯: {e}")
        return 1
    except KeyboardInterrupt:
        print(f"\n\nç”¨æˆ·ä¸­æ–­å¤„ç†")
        return 130
    except Exception as e:
        print(f"\næœªé¢„æœŸçš„é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)


def process_candecode_from_config(config_yaml_path: StringPathLike) -> int:
    """
    Convenience function to process CAN data using a config YAML file.
    Returns number of signals decoded successfully.
    
    Args:
        config_yaml_path: Path to the configuration YAML file
        
    Returns:
        Number of signals decoded
    """
    config_path = Path(config_yaml_path)
    if not config_path.exists():
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    
    decoder = CanDecoder.from_config(config_path)
    decoder.run_from_config()
    
    # Return count of output files as proxy for signals
    output_dir = Path(decoder.save_dir)
    if output_dir.exists():
        return len(list(output_dir.glob("*.parquet")))
    return 0

